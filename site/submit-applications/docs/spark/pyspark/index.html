
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-8.5.10">
    
    
      
        <title>Pyspark - EMR Containers Best Practices Guides</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.975780f9.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.2505c338.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="None" data-md-color-accent="None">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#pyspark-job-submission" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="EMR Containers Best Practices Guides" class="md-header__button md-logo" aria-label="EMR Containers Best Practices Guides" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            EMR Containers Best Practices Guides
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Pyspark
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/aws/aws-emr-containers-best-practices" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    aws/aws-emr-containers-best-practices
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        Guides
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../security/docs/spark/encryption/" class="md-tabs__link">
        Security
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="./" class="md-tabs__link md-tabs__link--active">
        Submit applications
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../storage/docs/spark/ebs/" class="md-tabs__link">
        Storage
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../metastore-integrations/docs/hive-metastore/" class="md-tabs__link">
        Metastore Integration
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../debugging/docs/change-log-level/" class="md-tabs__link">
        Debugging
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../troubleshoot/docs/troubleshooting/" class="md-tabs__link">
        Troubleshooting
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../node-placement/docs/eks-node-placement/" class="md-tabs__link">
        Node Placement
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../performance/docs/dra/" class="md-tabs__link">
        Performance
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../cost-optimization/docs/cost-optimization/" class="md-tabs__link">
        Cost Optimization
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="EMR Containers Best Practices Guides" class="md-nav__button md-logo" aria-label="EMR Containers Best Practices Guides" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    EMR Containers Best Practices Guides
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/aws/aws-emr-containers-best-practices" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    aws/aws-emr-containers-best-practices
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          Guides
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Guides" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Guides
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../outposts/emr-containers-on-outposts/" class="md-nav__link">
        EMR on EKS(AWS Outposts)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Security
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Security" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Security
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../security/docs/spark/encryption/" class="md-nav__link">
        Encryption
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../security/docs/spark/data-encryption/" class="md-nav__link">
        Data Encryption
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../security/docs/spark/network-security/" class="md-nav__link">
        Network Security
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../security/docs/spark/secrets/" class="md-nav__link">
        Secrets
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Submit applications
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Submit applications" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Submit applications
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Pyspark
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Pyspark
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#python-code-self-contained-in-a-single-py-file" class="md-nav__link">
    Python code self contained in a single .py file
  </a>
  
    <nav class="md-nav" aria-label="Python code self contained in a single .py file">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#python-file-from-s3" class="md-nav__link">
    Python file from S3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#python-file-from-mounted-volume" class="md-nav__link">
    Python file from mounted volume
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#python-code-with-python-dependencies" class="md-nav__link">
    Python code with python dependencies
  </a>
  
    <nav class="md-nav" aria-label="Python code with python dependencies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#list-of-py-files" class="md-nav__link">
    List of .py files
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-a-zip-file" class="md-nav__link">
    Bundled as a zip file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-a-egg-file" class="md-nav__link">
    Bundled as a .egg file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-a-whl-file" class="md-nav__link">
    Bundled as a .whl file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-a-pex-file" class="md-nav__link">
    Bundled as a .pex file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-a-targz-file-with-conda-pack" class="md-nav__link">
    Bundled as a tar.gz file with conda-pack
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-virtual-env" class="md-nav__link">
    Bundled as virtual env
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-docker-image" class="md-nav__link">
    Custom docker image
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#python-code-with-java-dependencies" class="md-nav__link">
    Python code with java dependencies
  </a>
  
    <nav class="md-nav" aria-label="Python code with java dependencies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#list-of-packages" class="md-nav__link">
    List of packages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#list-of-jar-files" class="md-nav__link">
    List of .jar files
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-docker-image_1" class="md-nav__link">
    Custom docker image
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#import-of-dynamic-modules-pyd-so" class="md-nav__link">
    Import of Dynamic Modules (.pyd, .so)
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Storage
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Storage" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Storage
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../storage/docs/spark/ebs/" class="md-nav__link">
        EBS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../storage/docs/spark/fsx-lustre/" class="md-nav__link">
        FSx for Lustre
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../storage/docs/spark/instance-store/" class="md-nav__link">
        Instance Store
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Metastore Integration
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Metastore Integration" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Metastore Integration
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../metastore-integrations/docs/hive-metastore/" class="md-nav__link">
        Hive Metastore
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../metastore-integrations/docs/aws-glue/" class="md-nav__link">
        AWS Glue
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          Debugging
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Debugging" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Debugging
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../debugging/docs/change-log-level/" class="md-nav__link">
        Change Log Level
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../debugging/docs/connect-spark-ui/" class="md-nav__link">
        Connect to Spark UI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../debugging/docs/self-hosted-shs/" class="md-nav__link">
        Self Hosted SHS
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Troubleshooting
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Troubleshooting" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Troubleshooting
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../troubleshoot/docs/troubleshooting/" class="md-nav__link">
        Troubleshooting EMR on EKS Issues
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_8">
          Node Placement
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Node Placement" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Node Placement
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../node-placement/docs/eks-node-placement/" class="md-nav__link">
        EKS Node placement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../node-placement/docs/fargate-node-placement/" class="md-nav__link">
        EKS Fargate Node placement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_9" type="checkbox" id="__nav_9" >
      
      
      
      
        <label class="md-nav__link" for="__nav_9">
          Performance
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Performance" data-md-level="1">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          Performance
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../performance/docs/dra/" class="md-nav__link">
        Dynamic Resource Allocation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../best-practices-and-recommendations/eks-best-practices/" class="md-nav__link">
        EKS Best Practices
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_10" type="checkbox" id="__nav_10" >
      
      
      
      
        <label class="md-nav__link" for="__nav_10">
          Cost Optimization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Cost Optimization" data-md-level="1">
        <label class="md-nav__title" for="__nav_10">
          <span class="md-nav__icon md-icon"></span>
          Cost Optimization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cost-optimization/docs/cost-optimization/" class="md-nav__link">
        Cost Optimization using EC2 Spot Instances
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cost-optimization/docs/node-decommission/" class="md-nav__link">
        Node Decommission
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#python-code-self-contained-in-a-single-py-file" class="md-nav__link">
    Python code self contained in a single .py file
  </a>
  
    <nav class="md-nav" aria-label="Python code self contained in a single .py file">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#python-file-from-s3" class="md-nav__link">
    Python file from S3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#python-file-from-mounted-volume" class="md-nav__link">
    Python file from mounted volume
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#python-code-with-python-dependencies" class="md-nav__link">
    Python code with python dependencies
  </a>
  
    <nav class="md-nav" aria-label="Python code with python dependencies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#list-of-py-files" class="md-nav__link">
    List of .py files
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-a-zip-file" class="md-nav__link">
    Bundled as a zip file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-a-egg-file" class="md-nav__link">
    Bundled as a .egg file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-a-whl-file" class="md-nav__link">
    Bundled as a .whl file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-a-pex-file" class="md-nav__link">
    Bundled as a .pex file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-a-targz-file-with-conda-pack" class="md-nav__link">
    Bundled as a tar.gz file with conda-pack
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bundled-as-virtual-env" class="md-nav__link">
    Bundled as virtual env
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-docker-image" class="md-nav__link">
    Custom docker image
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#python-code-with-java-dependencies" class="md-nav__link">
    Python code with java dependencies
  </a>
  
    <nav class="md-nav" aria-label="Python code with java dependencies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#list-of-packages" class="md-nav__link">
    List of packages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#list-of-jar-files" class="md-nav__link">
    List of .jar files
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-docker-image_1" class="md-nav__link">
    Custom docker image
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#import-of-dynamic-modules-pyd-so" class="md-nav__link">
    Import of Dynamic Modules (.pyd, .so)
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/aws/aws-emr-containers-best-practices/edit/master/docs/submit-applications/docs/spark/pyspark.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


<h1 id="pyspark-job-submission"><strong>Pyspark Job submission</strong><a class="headerlink" href="#pyspark-job-submission" title="Permanent link">&para;</a></h1>
<p>Python interpreter is bundled in the EMR containers spark image that is used to run the spark job.Python code and dependencies can be provided with the below options.</p>
<h3 id="python-code-self-contained-in-a-single-py-file">Python code self contained in a single .py file<a class="headerlink" href="#python-code-self-contained-in-a-single-py-file" title="Permanent link">&para;</a></h3>
<p>To start with, in the most simplest scenario - the example below shows how to submit a pi.py file that is self contained and doesn't need any other dependencies.    </p>
<h4 id="python-file-from-s3">Python file from S3<a class="headerlink" href="#python-file-from-s3" title="Permanent link">&para;</a></h4>
<p><strong>Request</strong>  <br />
pi.py used in the below request payload is from <a href="https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py">spark examples</a></p>
<div class="codehilite"><pre><span></span><code>cat &gt; spark-python-in-s3.json &lt;&lt; EOF
{
  &quot;name&quot;: &quot;spark-python-in-image&quot;, 
  &quot;virtualClusterId&quot;: &quot;&lt;virtual-cluster-id&gt;&quot;, 
  &quot;executionRoleArn&quot;: &quot;&lt;execution-role-arn&gt;&quot;, 
  &quot;releaseLabel&quot;: &quot;emr-6.2.0-latest&quot;, 
  &quot;jobDriver&quot;: {
    &quot;sparkSubmitJobDriver&quot;: {
      &quot;entryPoint&quot;: &quot;s3://&lt;s3 prefix&gt;/pi.py&quot;, 
       &quot;sparkSubmitParameters&quot;: &quot;--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4&quot;
    }
  }, 
  &quot;configurationOverrides&quot;: {
    &quot;applicationConfiguration&quot;: [
      {
        &quot;classification&quot;: &quot;spark-defaults&quot;, 
        &quot;properties&quot;: {
          &quot;spark.dynamicAllocation.enabled&quot;:&quot;false&quot;
         }
      }
    ], 
    &quot;monitoringConfiguration&quot;: {
      &quot;cloudWatchMonitoringConfiguration&quot;: {
        &quot;logGroupName&quot;: &quot;/emr-containers/jobs&quot;, 
        &quot;logStreamNamePrefix&quot;: &quot;demo&quot;
      }, 
      &quot;s3MonitoringConfiguration&quot;: {
        &quot;logUri&quot;: &quot;s3://joblogs&quot;
      }
    }
  }
}
EOF

aws emr-containers start-job-run --cli-input-json file:///Spark-Python-in-s3.json
</code></pre></div>

<h4 id="python-file-from-mounted-volume">Python file from mounted volume<a class="headerlink" href="#python-file-from-mounted-volume" title="Permanent link">&para;</a></h4>
<p>In the below example - pi.py is placed in a mounted volume. <a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html">FSx for Lustre filesystem</a> is mounted as a Persistent Volume on the driver pod under <code>/var/data/</code> and will be referenced by <code>local://</code> file prefix. For more information on how to mount FSx for lustre - <a href="../../../../storage/docs/spark/fsx-lustre/">EMR-Containers-integration-with-FSx-for-Lustre</a></p>
<blockquote>
<p>This approach can be used to provide spark application code and dependencies for execution. Persistent Volume mounted  to the driver and executor pods lets you access the application code and dependencies with <code>local://</code> prefix. </p>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">cat</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">spark</span><span class="o">-</span><span class="n">python</span><span class="o">-</span><span class="ow">in</span><span class="o">-</span><span class="n">FSx</span><span class="o">.</span><span class="n">json</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="n">EOF</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="s2">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;spark-python-in-FSx&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;virtualClusterId&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;virtual-cluster-id&gt;&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;executionRoleArn&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;execution-role-arn&gt;&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;releaseLabel&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;emr-6.2.0-latest&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;jobDriver&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;sparkSubmitJobDriver&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;entryPoint&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;local:///var/data/FSxLustre-pi.py&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">       </span><span class="s2">&quot;sparkSubmitParameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2&quot;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">},</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;configurationOverrides&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;applicationConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;classification&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;spark-defaults&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">        </span><span class="s2">&quot;properties&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.dynamicAllocation.enabled&quot;</span><span class="p">:</span><span class="s2">&quot;false&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName&quot;</span><span class="p">:</span><span class="s2">&quot;fsx-claim&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path&quot;</span><span class="p">:</span><span class="s2">&quot;/var/data/&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly&quot;</span><span class="p">:</span><span class="s2">&quot;false&quot;</span><span class="w"></span>
<span class="w">         </span><span class="p">}</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">],</span><span class="w"> </span>
<span class="w">    </span><span class="s2">&quot;monitoringConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;cloudWatchMonitoringConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;logGroupName&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/emr-containers/jobs&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">        </span><span class="s2">&quot;logStreamNamePrefix&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;demo&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">},</span><span class="w"> </span>
<span class="w">      </span><span class="s2">&quot;s3MonitoringConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;logUri&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;s3://joblogs&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="n">EOF</span><span class="w"></span>

<span class="n">aws</span><span class="w"> </span><span class="n">emr</span><span class="o">-</span><span class="n">containers</span><span class="w"> </span><span class="n">start</span><span class="o">-</span><span class="n">job</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">--</span><span class="n">cli</span><span class="o">-</span><span class="n">input</span><span class="o">-</span><span class="n">json</span><span class="w"> </span><span class="n">file</span><span class="p">:</span><span class="o">///</span><span class="n">Spark</span><span class="o">-</span><span class="n">Python</span><span class="o">-</span><span class="ow">in</span><span class="o">-</span><span class="n">Fsx</span><span class="o">.</span><span class="n">json</span><span class="w"></span>
</code></pre></div>

<h3 id="python-code-with-python-dependencies">Python code with python dependencies<a class="headerlink" href="#python-code-with-python-dependencies" title="Permanent link">&para;</a></h3>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p><a href="https://boto3.readthedocs.io/">boto3</a> will only work with '<a href="#bundled-as-a-pex-file">Bundled as a .pex file</a>' 
or with '<a href="#custom-docker-image">Custom docker image</a>'</p>
</div>
<h4 id="list-of-py-files"><strong>List of .py files</strong><a class="headerlink" href="#list-of-py-files" title="Permanent link">&para;</a></h4>
<p>This is not a scalable approach as the number of dependent files can grow to a large number, and also need to manually specify all of the transitive dependencies.  </p>
<div class="codehilite"><pre><span></span><code><span class="n">cat</span> <span class="o">&gt;</span> <span class="n">py</span><span class="o">-</span><span class="n">files</span><span class="o">-</span><span class="n">pi</span><span class="o">.</span><span class="n">py</span> <span class="o">&lt;&lt;</span><span class="n">EOF</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">add</span>

<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>

<span class="kn">import</span> <span class="nn">dependentFunc</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Usage: pi [partitions]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>
    <span class="n">partitions</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">2</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100000</span> <span class="o">*</span> <span class="n">partitions</span>

    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="n">count</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">partitions</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>
    <span class="n">dependentFunc</span><span class="o">.</span><span class="n">message</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pi is roughly </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="mf">4.0</span> <span class="o">*</span> <span class="n">count</span> <span class="o">/</span> <span class="n">n</span><span class="p">))</span>

    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

  <span class="n">EOF</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>cat &gt; dependentFunc.py &lt;&lt;EOF
def message():
  print(&quot;Printing from inside the dependent python file&quot;)

EOF
</code></pre></div>

<p>Upload dependentFunc.py and py-files-pi.py to s3  </p>
<p><strong>Request:</strong></p>
<div class="codehilite"><pre><span></span><code>cat &gt; spark-python-in-s3-dependency-files &lt;&lt; EOF
{
  &quot;name&quot;: &quot;spark-python-in-s3-dependency-files&quot;, 
  &quot;virtualClusterId&quot;: &quot;&lt;virtual-cluster-id&gt;&quot;, 
  &quot;executionRoleArn&quot;: &quot;&lt;execution-role-arn&gt;&quot;, 
  &quot;releaseLabel&quot;: &quot;emr-6.2.0-latest&quot;, 
  &quot;jobDriver&quot;: {
    &quot;sparkSubmitJobDriver&quot;: {
      &quot;entryPoint&quot;: &quot;s3://&lt;s3 prefix&gt;/py-files-pi.py&quot;, 
       &quot;sparkSubmitParameters&quot;: &quot;--py-files s3://&lt;s3 prefix&gt;/dependentFunc.py --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2&quot;
    }
  }, 
  &quot;configurationOverrides&quot;: {
    &quot;applicationConfiguration&quot;: [
      {
        &quot;classification&quot;: &quot;spark-defaults&quot;, 
        &quot;properties&quot;: {
          &quot;spark.dynamicAllocation.enabled&quot;:&quot;false&quot;
         }
      }
    ], 
    &quot;monitoringConfiguration&quot;: {
      &quot;cloudWatchMonitoringConfiguration&quot;: {
        &quot;logGroupName&quot;: &quot;/emr-containers/jobs&quot;, 
        &quot;logStreamNamePrefix&quot;: &quot;demo&quot;
      }, 
      &quot;s3MonitoringConfiguration&quot;: {
        &quot;logUri&quot;: &quot;s3://joblogs&quot;
      }
    }
  }
}
EOF

aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-dependency-files.json
</code></pre></div>

<h4 id="bundled-as-a-zip-file"><strong>Bundled as a zip file</strong><a class="headerlink" href="#bundled-as-a-zip-file" title="Permanent link">&para;</a></h4>
<p>In this approach all the dependent python files are bundled as a zip file.
Each folder should have <code>__init__.py</code> file as documented in  <a href="https://docs.python.org/3/reference/import.html#regular-packages">zip python dependencies</a>.
Zip should be done at the top folder level and using the -r option.</p>
<div class="codehilite"><pre><span></span><code>zip -r pyspark-packaged-dependency-src.zip . 
  adding: dependent/ (stored 0%)
  adding: dependent/__init__.py (stored 0%)
  adding: dependent/dependentFunc.py (deflated 7%)
</code></pre></div>

<p>dependentFunc.py from earlier example has been bundled as 
<a href="../../../resources/pyspark-packaged-dependency-src.zip">pyspark-packaged-dependency-src.zip</a>. Upload this file to a S3 location</p>
<div class="codehilite"><pre><span></span><code><span class="n">cat</span> <span class="o">&gt;</span> <span class="n">py</span><span class="o">-</span><span class="n">files</span><span class="o">-</span><span class="nb">zip</span><span class="o">-</span><span class="n">pi</span><span class="o">.</span><span class="n">py</span> <span class="o">&lt;&lt;</span><span class="n">EOF</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">add</span>

<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>

<span class="o">**</span><span class="kn">from</span> <span class="nn">dependent</span> <span class="kn">import</span> <span class="n">dependentFunc</span><span class="o">**</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Usage: pi [partitions]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>
    <span class="n">partitions</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">2</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100000</span> <span class="o">*</span> <span class="n">partitions</span>

    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="n">count</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">partitions</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>
    <span class="n">dependentFunc</span><span class="o">.</span><span class="n">message</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pi is roughly </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="mf">4.0</span> <span class="o">*</span> <span class="n">count</span> <span class="o">/</span> <span class="n">n</span><span class="p">))</span>

    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
  <span class="n">EOF</span>
</code></pre></div>

<p><strong>Request:</strong></p>
<div class="codehilite"><pre><span></span><code>cat &gt; spark-python-in-s3-dependency-zip.json &lt;&lt;EOF
{
  &quot;name&quot;: &quot;spark-python-in-s3-dependency-zip&quot;, 
  &quot;virtualClusterId&quot;: &quot;&lt;virtual-cluster-id&gt;&quot;, 
  &quot;executionRoleArn&quot;: &quot;&lt;execution-role-arn&gt;&quot;, 
  &quot;releaseLabel&quot;: &quot;emr-6.2.0-latest&quot;, 
  &quot;jobDriver&quot;: {
    &quot;sparkSubmitJobDriver&quot;: {
      &quot;entryPoint&quot;: &quot;s3://&lt;s3 prefix&gt;/py-files-zip-pi.py&quot;, 
       &quot;sparkSubmitParameters&quot;: &quot;--py-files s3://&lt;s3 prefix&gt;/pyspark-packaged-dependency-src.zip --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2&quot;
    }
  }, 
  &quot;configurationOverrides&quot;: {
    &quot;applicationConfiguration&quot;: [
      {
        &quot;classification&quot;: &quot;spark-defaults&quot;, 
        &quot;properties&quot;: {
          &quot;spark.dynamicAllocation.enabled&quot;:&quot;false&quot;
          }
      }
    ], 
    &quot;monitoringConfiguration&quot;: {
      &quot;cloudWatchMonitoringConfiguration&quot;: {
        &quot;logGroupName&quot;: &quot;/emr-containers/jobs&quot;, 
        &quot;logStreamNamePrefix&quot;: &quot;demo&quot;
      }, 
      &quot;s3MonitoringConfiguration&quot;: {
        &quot;logUri&quot;: &quot;s3://joblogs&quot;
      }
    }
  }
}
EOF

aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-dependency-zip.json
</code></pre></div>

<h4 id="bundled-as-a-egg-file"><strong>Bundled as a .egg file</strong><a class="headerlink" href="#bundled-as-a-egg-file" title="Permanent link">&para;</a></h4>
<p>Create a folder structure as in the below screenshot with the code from the previous example - <code>py-files-zip-pi.py, dependentFunc.py</code>
<img alt="" src="../../../resources/images/pyspark-packaged-example-zip-folder-structure.png" />  </p>
<p>Steps to create .egg file</p>
<div class="codehilite"><pre><span></span><code><span class="n">cd</span><span class="w"> </span><span class="o">/</span><span class="n">pyspark</span><span class="o">-</span><span class="n">packaged</span><span class="o">-</span><span class="n">example</span><span class="w"></span>
<span class="n">pip</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">setuptools</span><span class="w"></span>
<span class="n">python</span><span class="w"> </span><span class="n">setup</span><span class="o">.</span><span class="n">py</span><span class="w"> </span><span class="n">bdist_egg</span><span class="w"></span>
</code></pre></div>

<p>Upload <code>dist/pyspark_packaged_example-0.0.3-py3.8.egg</code> to a S3 location  </p>
<p><strong>Request:</strong></p>
<div class="codehilite"><pre><span></span><code>cat &gt; spark-python-in-s3-dependency-egg.json &lt;&lt;EOF
{
  &quot;name&quot;: &quot;spark-python-in-s3-dependency-egg&quot;, 
  &quot;virtualClusterId&quot;: &quot;&lt;virtual-cluster-id&gt;&quot;, 
  &quot;executionRoleArn&quot;: &quot;&lt;execution-role-arn&gt;&quot;, 
  &quot;releaseLabel&quot;: &quot;emr-6.2.0-latest&quot;, 
  &quot;jobDriver&quot;: {
    &quot;sparkSubmitJobDriver&quot;: {
      &quot;entryPoint&quot;: &quot;s3://&lt;s3 prefix&gt;/py-files-zip-pi.py&quot;, 
       &quot;sparkSubmitParameters&quot;: &quot;--py-files s3://&lt;s3 prefix&gt;/pyspark_packaged_example-0.0.3-py3.8.egg --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2&quot;
    }
  }, 
  &quot;configurationOverrides&quot;: {
    &quot;applicationConfiguration&quot;: [
      {
        &quot;classification&quot;: &quot;spark-defaults&quot;, 
        &quot;properties&quot;: {
          &quot;spark.dynamicAllocation.enabled&quot;:&quot;false&quot;
         }
      }
    ], 
    &quot;monitoringConfiguration&quot;: {
      &quot;cloudWatchMonitoringConfiguration&quot;: {
        &quot;logGroupName&quot;: &quot;/emr-containers/jobs&quot;, 
        &quot;logStreamNamePrefix&quot;: &quot;demo&quot;
      }, 
      &quot;s3MonitoringConfiguration&quot;: {
        &quot;logUri&quot;: &quot;s3://joblogs&quot;
      }
    }
  }
}
EOF

aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-dependency-egg.json
</code></pre></div>

<h4 id="bundled-as-a-whl-file"><strong>Bundled as a .whl file</strong><a class="headerlink" href="#bundled-as-a-whl-file" title="Permanent link">&para;</a></h4>
<p>Create a folder structure as in the below screenshot with the code from the previous example - py-files-zip-pi.py, dependentFunc.py
<img alt="" src="../../../resources/images/pyspark-packaged-example-zip-folder-structure.png" />   </p>
<p>Steps to create .whl file</p>
<div class="codehilite"><pre><span></span><code>cd /pyspark-packaged-example
`pip install wheel`
python setup.py bdist_wheel
</code></pre></div>

<p>Upload <code>dist/pyspark_packaged_example-0.0.3-py3-none-any.whl</code> to a s3 location</p>
<p><strong>Request:</strong></p>
<div class="codehilite"><pre><span></span><code>cat &gt; spark-python-in-s3-dependency-wheel.json &lt;&lt;EOF
{
  &quot;name&quot;: &quot;spark-python-in-s3-dependency-wheel&quot;, 
  &quot;virtualClusterId&quot;: &quot;&lt;virtual-cluster-id&gt;&quot;, 
  &quot;executionRoleArn&quot;: &quot;&lt;execution-role-arn&gt;&quot;, 
  &quot;releaseLabel&quot;: &quot;emr-6.2.0-latest&quot;, 
  &quot;jobDriver&quot;: {
    &quot;sparkSubmitJobDriver&quot;: {
      &quot;entryPoint&quot;: &quot;s3://&lt;s3 prefix&gt;/py-files-zip-pi.py&quot;, 
       &quot;sparkSubmitParameters&quot;: &quot;--py-files s3://&lt;s3 prefix&gt;/pyspark_packaged_example-0.0.3-py3-none-any.whl --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2&quot;
    }
  }, 
  &quot;configurationOverrides&quot;: {
    &quot;applicationConfiguration&quot;: [
      {
        &quot;classification&quot;: &quot;spark-defaults&quot;, 
        &quot;properties&quot;: {
          &quot;spark.dynamicAllocation.enabled&quot;:&quot;false&quot;
         }
      }
    ], 
    &quot;monitoringConfiguration&quot;: {
      &quot;cloudWatchMonitoringConfiguration&quot;: {
        &quot;logGroupName&quot;: &quot;/emr-containers/jobs&quot;, 
        &quot;logStreamNamePrefix&quot;: &quot;demo&quot;
      }, 
      &quot;s3MonitoringConfiguration&quot;: {
        &quot;logUri&quot;: &quot;s3://joblogs&quot;
      }
    }
  }
}
EOF

aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-dependency-wheel.json
</code></pre></div>

<h4 id="bundled-as-a-pex-file">Bundled as a .pex file<a class="headerlink" href="#bundled-as-a-pex-file" title="Permanent link">&para;</a></h4>
<p><a href="https://github.com/pantsbuild/pex">pex</a> is a library for generating .pex (Python EXecutable) files which are executable Python environments.PEX files can be created as below</p>
<div class="codehilite"><pre><span></span><code>docker run -it -v $(pwd):/workdir python:3.7.9-buster /bin/bash #python 3.7.9 is installed in EMR 6.1.0
pip3 install pex
pex --python=python3 --inherit-path=prefer -v numpy -o numpy_dep.pex
</code></pre></div>

<p>To read more about PEX:
<a href="https://github.com/pantsbuild/pex">PEX</a>
<a href="https://readthedocs.org/projects/manypex/downloads/pdf/latest/">PEX documentation</a>
<a href="http://www.legendu.net/misc/blog/tips-on-pex/">Tips on PEX</a>
<a href="http://www.legendu.net/misc/blog/packaging-python-dependencies-for-pyspark-using-pex/">pex packaging for pyspark</a>  </p>
<p><strong>Approach 1: Using Persistent Volume - FSx for Lustre cluster</strong></p>
<p>Upload <code>numpy_dep.pex</code> to a s3 location that is mapped to a FSx for Lustre cluster. <code>numpy_dep.pex</code> can be placed on any Kubernetes persistent volume and mounted to the driver pod and executor pod.<br />
<strong>Request:</strong>
<code>kmeans.py</code> used in the below request is from <a href="https://github.com/apache/spark/blob/master/examples/src/main/python/kmeans.py">spark examples</a></p>
<div class="codehilite"><pre><span></span><code><span class="n">cat</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">spark</span><span class="o">-</span><span class="n">python</span><span class="o">-</span><span class="ow">in</span><span class="o">-</span><span class="n">s3</span><span class="o">-</span><span class="n">pex</span><span class="o">-</span><span class="n">fsx</span><span class="o">.</span><span class="n">json</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">EOF</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="s2">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;spark-python-in-s3-pex-fsx&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;virtualClusterId&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;virtual-cluster-id&gt;&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;executionRoleArn&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;execution-role-arn&gt;&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;releaseLabel&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;emr-6.2.0-latest&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;jobDriver&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;sparkSubmitJobDriver&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;entryPoint&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;s3://&lt;s3 prefix&gt;/kmeans.py&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;entryPointArguments&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;s3://&lt;s3 prefix&gt;/kmeans_data.txt&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;2&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;3&quot;</span><span class="w"></span>
<span class="w">       </span><span class="p">],</span><span class="w"> </span>
<span class="w">       </span><span class="s2">&quot;sparkSubmitParameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2&quot;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">},</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;configurationOverrides&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;applicationConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;classification&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;spark-defaults&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">        </span><span class="s2">&quot;properties&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.pyspark.pythonVersion&quot;</span><span class="p">:</span><span class="s2">&quot;3&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driverEnv.PEX_ROOT&quot;</span><span class="p">:</span><span class="s2">&quot;./tmp&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.executorEnv.PEX_ROOT&quot;</span><span class="p">:</span><span class="s2">&quot;./tmp&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driverEnv.PEX_INHERIT_PATH&quot;</span><span class="p">:</span><span class="s2">&quot;prefer&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.executorEnv.PEX_INHERIT_PATH&quot;</span><span class="p">:</span><span class="s2">&quot;prefer&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driverEnv.PEX_VERBOSE&quot;</span><span class="p">:</span><span class="s2">&quot;10&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driverEnv.PEX_PYTHON&quot;</span><span class="p">:</span><span class="s2">&quot;python3&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.executorEnv.PEX_PYTHON&quot;</span><span class="p">:</span><span class="s2">&quot;python3&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.pyspark.driver.python&quot;</span><span class="p">:</span><span class="s2">&quot;/var/data/numpy_dep.pex&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.pyspark.python&quot;</span><span class="p">:</span><span class="s2">&quot;/var/data/numpy_dep.pex&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName&quot;</span><span class="p">:</span><span class="s2">&quot;fsx-claim&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path&quot;</span><span class="p">:</span><span class="s2">&quot;/var/data/&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly&quot;</span><span class="p">:</span><span class="s2">&quot;false&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName&quot;</span><span class="p">:</span><span class="s2">&quot;fsx-claim&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path&quot;</span><span class="p">:</span><span class="s2">&quot;/var/data/&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly&quot;</span><span class="p">:</span><span class="s2">&quot;false&quot;</span><span class="w"></span>
<span class="w">         </span><span class="p">}</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">],</span><span class="w"> </span>
<span class="w">    </span><span class="s2">&quot;monitoringConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span>
<span class="w">      </span><span class="s2">&quot;cloudWatchMonitoringConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;logGroupName&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/emr-containers/jobs&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">        </span><span class="s2">&quot;logStreamNamePrefix&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;demo&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">},</span><span class="w"> </span>
<span class="w">      </span><span class="s2">&quot;s3MonitoringConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;logUri&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;s3://joblogs&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="n">aws</span><span class="w"> </span><span class="n">emr</span><span class="o">-</span><span class="n">containers</span><span class="w"> </span><span class="n">start</span><span class="o">-</span><span class="n">job</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">--</span><span class="n">cli</span><span class="o">-</span><span class="n">input</span><span class="o">-</span><span class="n">json</span><span class="w"> </span><span class="n">file</span><span class="p">:</span><span class="o">////</span><span class="n">Spark</span><span class="o">-</span><span class="n">Python</span><span class="o">-</span><span class="ow">in</span><span class="o">-</span><span class="n">s3</span><span class="o">-</span><span class="n">pex</span><span class="o">-</span><span class="n">fsx</span><span class="o">.</span><span class="n">json</span><span class="w"></span>
</code></pre></div>

<p><strong>Approach 2: Using Custom Pod Templates</strong></p>
<p>Upload <code>numpy_dep.pex</code> to a s3 location. Create <a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/pod-templates.html">custom pod templates</a> for driver and executor pods. Custom pod templates allows running a command through <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">initContainers</a> before the main application container is created.
In this case, the command will download the <code>numpy_dep.pex</code> file to the <code>/tmp/numpy_dep.pex</code> path of the driver and executor pods.</p>
<p>Note: This approach is only supported for release image 5.33.0 and later or 6.3.0 and later.</p>
<p>Sample driver pod template YAML file:</p>
<div class="codehilite"><pre><span></span><code>cat &gt; driver_pod_tenplate.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
spec:
 containers:
   - name: spark-kubernetes-driver
 initContainers: 
   - name: my-init-container
     image: 895885662937.dkr.ecr.us-west-2.amazonaws.com/spark/emr-5.33.0-20210323:2.4.7-amzn-1-vanilla
     volumeMounts:
       - name: temp-data-dir
         mountPath: /tmp
     command:
       - sh
       - -c
       - aws s3api get-object --bucket &lt;s3-bucket&gt; --key &lt;s3-key-prefix&gt;/numpy_dep.pex /tmp/numpy_dep.pex &amp;&amp; chmod u+x /tmp/numpy_dep.pex
EOF
</code></pre></div>

<p>Sample executor pod template YAML file:</p>
<div class="codehilite"><pre><span></span><code>cat &gt; executor_pod_tenplate.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
spec:
  containers:
    - name: spark-kubernetes-executor
  initContainers: 
    - name: my-init-container
      image: 895885662937.dkr.ecr.us-west-2.amazonaws.com/spark/emr-5.33.0-20210323:2.4.7-amzn-1-vanilla
      volumeMounts:
        - name: temp-data-dir
          mountPath: /tmp
      command:
        - sh
        - -c
        - aws s3api get-object --bucket &lt;s3-bucket&gt; --key &lt;s3-key-prefix&gt;/numpy_dep.pex /tmp/numpy_dep.pex &amp;&amp; chmod u+x /tmp/numpy_dep.pex
EOF
</code></pre></div>

<p>Replace initContainer's <code>image</code> with the respective release label's container image. In this case we are using the image of release <code>emr-5.33.0-latest</code>.
Upload the driver and executor custom pod templates to S3</p>
<p><strong>Request:</strong>
<code>kmeans.py</code> used in the below request is from <a href="https://github.com/apache/spark/blob/master/examples/src/main/python/kmeans.py">spark examples</a></p>
<div class="codehilite"><pre><span></span><code>cat &gt; spark-python-in-s3-pex-pod-templates.json &lt;&lt; EOF
{
  &quot;name&quot;: &quot;spark-python-in-s3-pex-pod-templates&quot;, 
  &quot;virtualClusterId&quot;: &quot;&lt;virtual-cluster-id&gt;&quot;, 
  &quot;executionRoleArn&quot;: &quot;&lt;execution-role-arn&gt;&quot;, 
  &quot;releaseLabel&quot;: &quot;emr-5.33.0-latest&quot;, 
  &quot;jobDriver&quot;: {
    &quot;sparkSubmitJobDriver&quot;: {
      &quot;entryPoint&quot;: &quot;s3://&lt;s3 prefix&gt;/kmeans.py&quot;,
      &quot;entryPointArguments&quot;: [
        &quot;s3://&lt;s3 prefix&gt;/kmeans_data.txt&quot;,
        &quot;2&quot;,
        &quot;3&quot;
       ], 
       &quot;sparkSubmitParameters&quot;: &quot;--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2&quot;
    }
  }, 
  &quot;configurationOverrides&quot;: {
    &quot;applicationConfiguration&quot;: [
      {
        &quot;classification&quot;: &quot;spark-defaults&quot;, 
        &quot;properties&quot;: {
          &quot;spark.kubernetes.pyspark.pythonVersion&quot;:&quot;3&quot;,
          &quot;spark.kubernetes.driverEnv.PEX_ROOT&quot;:&quot;./tmp&quot;,
          &quot;spark.executorEnv.PEX_ROOT&quot;:&quot;./tmp&quot;,
          &quot;spark.kubernetes.driverEnv.PEX_INHERIT_PATH&quot;:&quot;prefer&quot;,
          &quot;spark.executorEnv.PEX_INHERIT_PATH&quot;:&quot;prefer&quot;,
          &quot;spark.kubernetes.driverEnv.PEX_VERBOSE&quot;:&quot;10&quot;,
          &quot;spark.kubernetes.driverEnv.PEX_PYTHON&quot;:&quot;python3&quot;,
          &quot;spark.executorEnv.PEX_PYTHON&quot;:&quot;python3&quot;,
          &quot;spark.pyspark.driver.python&quot;:&quot;/tmp/numpy_dep.pex&quot;,
          &quot;spark.pyspark.python&quot;:&quot;/tmp/numpy_dep.pex&quot;,
          &quot;spark.kubernetes.driver.podTemplateFile&quot;: &quot;s3://&lt;s3-prefix&gt;/driver_pod_template.yaml&quot;,
          &quot;spark.kubernetes.executor.podTemplateFile&quot;: &quot;s3://&lt;s3-prefix&gt;/executor_pod_template.yaml&quot;
         }
      }
    ], 
    &quot;monitoringConfiguration&quot;: { 
      &quot;cloudWatchMonitoringConfiguration&quot;: {
        &quot;logGroupName&quot;: &quot;/emr-containers/jobs&quot;, 
        &quot;logStreamNamePrefix&quot;: &quot;demo&quot;
      }, 
      &quot;s3MonitoringConfiguration&quot;: {
        &quot;logUri&quot;: &quot;s3://joblogs&quot;
      }
    }
  }
}

aws emr-containers start-job-run --cli-input-json file:////Spark-Python-in-s3-pex-pod-templates.json
</code></pre></div>

<p><strong>Point to Note:</strong><br />
PEX files don’t have the python interpreter bundled with it. Using the PEX env variables, we pass in the python interpreter installed in the spark driver and executor docker image.</p>
<blockquote>
<p>pex vs conda-pack
A pex file contain only dependent Python packages but not a Python interpreter in it while a conda-pack environment has a Python interpreter as well, so with the same Python packages a conda-pack environment is much larger than a pex file.
A conda-pack environment is a tar.gz file and need to be decompressed before being used while a pex file can be used directly.
If a Python interpreter exists, pex is a better option than conda-pack. However, conda-pack is the ONLY CHOICE if you need a specific version of Python interpreter which does not exist and you do not have permission to install one (e.g., when you need to use a specific version of Python interpreter with an enterprise PySpark cluster). If the pex file or conda-pack environment needs to be distributed to machines on demand, there are some overhead before running your application. With the same Python packages, a conda-pack environment has large overhead/latency than the pex file as the conda-pack environment is usually much larger and need to be decompressed before being used.</p>
</blockquote>
<p>For more information - <a href="http://www.legendu.net/misc/blog/tips-on-pex/">Tips on PEX</a> </p>
<h4 id="bundled-as-a-targz-file-with-conda-pack">Bundled as a tar.gz file with conda-pack<a class="headerlink" href="#bundled-as-a-targz-file-with-conda-pack" title="Permanent link">&para;</a></h4>
<p><a href="https://conda.github.io/conda-pack/spark.html">conda-pack for spark</a>
Install conda through <a href="https://conda.io/miniconda.html">Miniconda</a>
Open a new terminal and execute the below commands</p>
<div class="codehilite"><pre><span></span><code>conda create -y -n example python=3.5 numpy
conda activate example
pip install conda-pack
conda pack -f -o numpy_environment.tar.gz
</code></pre></div>

<p>Upload <code>numpy_environment.tar.gz</code> to a s3 location that is mapped to a FSx for Lustre cluster. <code>numpy_environment.tar.gz</code> can be placed on any Kubernetes persistent volume and mounted to the driver pod and executor pod.Alternatively, S3 path for <code>numpy_environment.tar.gz</code> can also be passed using <a href="# List of .py files">--py-files</a>  </p>
<p><strong>Request:</strong></p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="s2">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;spark-python-in-s3-conda-fsx&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;virtualClusterId&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;virtual-cluster-id&gt;&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;executionRoleArn&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;execution-role-arn&gt;&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;releaseLabel&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;emr-6.2.0-latest&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;jobDriver&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;sparkSubmitJobDriver&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;entryPoint&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;s3://&lt;s3 prefix&gt;/kmeans.py&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;entryPointArguments&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;s3://&lt;s3 prefix&gt;/kmeans_data.txt&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;2&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;3&quot;</span><span class="w"></span>
<span class="w">       </span><span class="p">],</span><span class="w"> </span>
<span class="w">       </span><span class="s2">&quot;sparkSubmitParameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--verbose --archives /var/data/numpy_environment.tar.gz#environment --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4&quot;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">},</span><span class="w"> </span>
<span class="w">  </span><span class="s2">&quot;configurationOverrides&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;applicationConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;classification&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;spark-defaults&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">        </span><span class="s2">&quot;properties&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.executor.instances&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.dynamicAllocation.enabled&quot;</span><span class="p">:</span><span class="s2">&quot;false&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.files&quot;</span><span class="p">:</span><span class="s2">&quot;/var/data/numpy_environment.tar.gz#environment&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.pyspark.pythonVersion&quot;</span><span class="p">:</span><span class="s2">&quot;3&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.pyspark.driver.python&quot;</span><span class="p">:</span><span class="s2">&quot;./environment/bin/python&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.pyspark.python&quot;</span><span class="p">:</span><span class="s2">&quot;./environment/bin/python&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.options.claimName&quot;</span><span class="p">:</span><span class="s2">&quot;fsx-claim&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.path&quot;</span><span class="p">:</span><span class="s2">&quot;/var/data/&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.sparkdata.mount.readOnly&quot;</span><span class="p">:</span><span class="s2">&quot;false&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.options.claimName&quot;</span><span class="p">:</span><span class="s2">&quot;fsx-claim&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.path&quot;</span><span class="p">:</span><span class="s2">&quot;/var/data/&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="s2">&quot;spark.kubernetes.executor.volumes.persistentVolumeClaim.sparkdata.mount.readOnly&quot;</span><span class="p">:</span><span class="s2">&quot;false&quot;</span><span class="w"></span>
<span class="w">         </span><span class="p">}</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">],</span><span class="w"> </span>
<span class="w">    </span><span class="s2">&quot;monitoringConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;cloudWatchMonitoringConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;logGroupName&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/emr-containers/jobs&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">        </span><span class="s2">&quot;logStreamNamePrefix&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;demo&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">},</span><span class="w"> </span>
<span class="w">      </span><span class="s2">&quot;s3MonitoringConfiguration&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">&quot;logUri&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;s3://joblogs&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>

<p><strong>The above request doesn't work with spark on kubernetes</strong></p>
<h4 id="bundled-as-virtual-env">Bundled as virtual env<a class="headerlink" href="#bundled-as-virtual-env" title="Permanent link">&para;</a></h4>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>This will not work with spark on kubernetes</strong></p>
</div>
<p>This feature only works with YARN - cluster mode
In this implementation for YARN - the dependencies will be installed from the repository for every driver and executor. This might not be a more scalable model as per <a href="https://issues.apache.org/jira/browse/SPARK-25433">SPARK-25433</a>. Recommended solution is to pass in the dependencies as PEX file.</p>
<h4 id="custom-docker-image">Custom docker image<a class="headerlink" href="#custom-docker-image" title="Permanent link">&para;</a></h4>
<p>See the details in the <a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/docker-custom-images.html">official documentation</a>.  </p>
<p><strong>Dockerfile</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">FROM</span><span class="w"> </span><span class="s">107292555468.dkr.ecr.eu-central-1.amazonaws.com/spark/emr-6.3.0</span>
<span class="k">USER</span><span class="w"> </span><span class="s">root</span>
<span class="k">RUN</span><span class="w"> </span>pip3 install boto3
<span class="k">USER</span><span class="w"> </span><span class="s">hadoop:hadoop</span>
</code></pre></div>

<h3 id="python-code-with-java-dependencies">Python code with java dependencies<a class="headerlink" href="#python-code-with-java-dependencies" title="Permanent link">&para;</a></h3>
<h4 id="list-of-packages"><strong>List of packages</strong><a class="headerlink" href="#list-of-packages" title="Permanent link">&para;</a></h4>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>This will not work with spark on kubernetes</strong></p>
</div>
<p>This feature only works with YARN - cluster mode</p>
<p>kafka integration example</p>
<div class="codehilite"><pre><span></span><code>./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2
</code></pre></div>

<h4 id="list-of-jar-files"><strong>List of .jar files</strong><a class="headerlink" href="#list-of-jar-files" title="Permanent link">&para;</a></h4>
<p>This is not a scalable approach as the number of dependent files can grow to a large number, and also need to manually specify all of the transitive dependencies.</p>
<p>How to find all of the .jar files which belongs to given package?</p>
<ol>
<li>Go to <a href="https://mvnrepository.com/">Maven Repository</a></li>
<li>Search for the package name</li>
<li>Select the matching Spark and Scala version</li>
<li>Copy the URL of the jar file</li>
<li>Copy the URL of the jar file of all compile dependencies</li>
</ol>
<p><strong><em>Request:</em></strong></p>
<div class="codehilite"><pre><span></span><code>cat &gt; Spark-Python-with-jars.json <span class="s">&lt;&lt; EOF</span>
<span class="s">{</span>
<span class="s">  &quot;name&quot;: &quot;spark-python-with-jars&quot;,</span>
<span class="s">  &quot;virtualClusterId&quot;: &quot;&lt;virtual-cluster-id&gt;&quot;,</span>
<span class="s">  &quot;executionRoleArn&quot;: &quot;&lt;execution-role-arn&gt;&quot;,</span>
<span class="s">  &quot;releaseLabel&quot;: &quot;emr-6.2.0-latest&quot;,</span>
<span class="s">  &quot;jobDriver&quot;: {</span>
<span class="s">    &quot;sparkSubmitJobDriver&quot;: {</span>
<span class="s">      &quot;entryPoint&quot;: &quot;s3://&lt;s3 prefix&gt;/pi.py&quot;,</span>
<span class="s">      &quot;sparkSubmitParameters&quot;: &quot;--jars https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.1.1/spark-sql-kafka-0-10_2.12-3.1.1.jar,https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar,https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.6.0/kafka-clients-2.6.0.jar,https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.1.1/spark-token-provider-kafka-0-10_2.12-3.1.1.jar,https://repo1.maven.org/maven2/org/apache/spark/spark-tags_2.12/3.1.1/spark-tags_2.12-3.1.1.jar --conf spark.driver.cores=3 --conf spark.executor.memory=8G --conf spark.driver.memory=6G --conf spark.executor.cores=3&quot;</span>
<span class="s">    }</span>
<span class="s">  },</span>
<span class="s">  &quot;configurationOverrides&quot;: {</span>
<span class="s">    &quot;monitoringConfiguration&quot;: {</span>
<span class="s">      &quot;cloudWatchMonitoringConfiguration&quot;: {</span>
<span class="s">        &quot;logGroupName&quot;: &quot;/emr-containers/jobs&quot;,</span>
<span class="s">        &quot;logStreamNamePrefix&quot;: &quot;demo&quot;</span>
<span class="s">      },</span>
<span class="s">      &quot;s3MonitoringConfiguration&quot;: {</span>
<span class="s">        &quot;logUri&quot;: &quot;s3://joblogs&quot;</span>
<span class="s">      }</span>
<span class="s">    }</span>
<span class="s">  }</span>
<span class="s">}</span>
<span class="s">EOF</span>

aws emr-containers start-job-run --cli-input-json file:///Spark-Python-with-jars.json
</code></pre></div>

<h4 id="custom-docker-image_1">Custom docker image<a class="headerlink" href="#custom-docker-image_1" title="Permanent link">&para;</a></h4>
<p>See the basics in the <a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/docker-custom-images.html">official documentation</a>.  </p>
<p><strong>Approach 1: List of .jar files</strong>  </p>
<p>This is not a scalable approach as the number of dependent files can grow to a large number, and also need to manually specify all of the transitive dependencies.</p>
<p>How to find all of the .jar files which belongs to given package?</p>
<ol>
<li>Go to <a href="https://mvnrepository.com/">Maven Repository</a></li>
<li>Search for the package name</li>
<li>Select the matching Spark and Scala version</li>
<li>Copy the URL of the jar file</li>
<li>Copy the URL of the jar file of all compile dependencies</li>
</ol>
<p><strong>Dockerfile</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">FROM</span><span class="w"> </span><span class="s">107292555468.dkr.ecr.eu-central-1.amazonaws.com/spark/emr-6.3.0</span>

<span class="k">USER</span><span class="w"> </span><span class="s">root</span>

<span class="k">ARG</span><span class="w"> </span><span class="nv">JAR_HOME</span><span class="o">=</span>/usr/lib/spark/jars/

<span class="c"># Kafka</span>
<span class="k">ADD</span><span class="w"> </span>https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.1.1/spark-sql-kafka-0-10_2.12-3.1.1.jar <span class="nv">$JAR_HOME</span>
<span class="k">ADD</span><span class="w"> </span>https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar <span class="nv">$JAR_HOME</span>
<span class="k">ADD</span><span class="w"> </span>https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.6.0/kafka-clients-2.6.0.jar <span class="nv">$JAR_HOME</span>
<span class="k">ADD</span><span class="w"> </span>https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.1.1/spark-token-provider-kafka-0-10_2.12-3.1.1.jar <span class="nv">$JAR_HOME</span>
<span class="k">ADD</span><span class="w"> </span>https://repo1.maven.org/maven2/org/apache/spark/spark-tags_2.12/3.1.1/spark-tags_2.12-3.1.1.jar <span class="nv">$JAR_HOME</span>

<span class="k">RUN</span><span class="w"> </span>chmod -R +r  /usr/lib/spark/jars

<span class="k">USER</span><span class="w"> </span><span class="s">hadoop:hadoop</span>
</code></pre></div>

<p><strong>Observed Behavior:</strong><br />
Spark automatically installs all the .jar files from <code>/usr/lib/spark/jars/</code> directory. In Dockerfile we are adding these 
file as root user and these file will get <code>-rw-------</code> permission while the original files have <code>-rw-r--r--</code> permission. 
EMR on EKS uses hadoop:hadoop to run spark jobs and files with <code>-rw-------</code> permission are hidden from this user and can 
not be imported. To make these file readable for all the users run the following command <code>chmod -R +r /usr/lib/spark/jars</code>
and the files will have <code>-rw-r--r--</code> permission.</p>
<p><br></br>
<strong>Approach 2: List of packages</strong>  </p>
<p>This approach is a resource intensive (min 1vCPU, 2GB RAM) solution, because it will run a dummy spark job. Scale your local or CI/CD resources
according to it. </p>
<p><strong>Dockerfile</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">FROM</span><span class="w"> </span><span class="s">107292555468.dkr.ecr.eu-central-1.amazonaws.com/spark/emr-6.3.0</span>

<span class="k">USER</span><span class="w"> </span><span class="s">root</span>

<span class="k">ARG</span><span class="w"> </span><span class="nv">KAFKA_PKG</span><span class="o">=</span><span class="s2">&quot;org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2&quot;</span>

<span class="k">RUN</span><span class="w"> </span>spark-submit run-example --packages <span class="nv">$KAFKA_PKG</span> --deploy-mode<span class="o">=</span>client --master<span class="o">=</span>local<span class="o">[</span><span class="m">1</span><span class="o">]</span> SparkPi
<span class="k">RUN</span><span class="w"> </span>mv /root/.ivy2/jars/* /usr/lib/spark/jars/

<span class="k">USER</span><span class="w"> </span><span class="s">hadoop:hadoop</span>
</code></pre></div>

<p><strong>Observed Behavior:</strong> <br />
Spark runs <a href="https://ant.apache.org/ivy/">ivy</a> to get all of its dependencies (packages) when <code>--packages</code> are defined in the submit command.
We can run a "dummy" spark job to make spark downloads its packages. These .jars are saved in <code>/root/.ivy2/jars/</code> which 
we can move to <code>/usr/lib/spark/jars/</code> for further use. These jars having <code>-rw-r--r--</code> permission and does not require further modifications.
The advantage of this method is ivy download the dependencies of the package as well, and we needed to specify only 
<code>org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2</code> instead of 5 jars files above.</p>
<h3 id="import-of-dynamic-modules-pyd-so"><strong>Import of Dynamic Modules (.pyd, .so)</strong><a class="headerlink" href="#import-of-dynamic-modules-pyd-so" title="Permanent link">&para;</a></h3>
<p>Import of dynamic modules(.pyd, .so) is <a href="https://docs.python.org/3/library/zipimport.html#module-zipimport"><strong>disallowed when bundled as a zip</strong></a>  </p>
<p>Steps to create a .so file<br />
<strong>example.c</strong></p>
<div class="codehilite"><pre><span></span><code><span class="cm">/* File : example.c */</span><span class="w"></span>

<span class="w"> </span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;example.h&quot;</span><span class="cp"></span>
<span class="w"> </span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"></span>
<span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s"> Inside add function in C library </span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="p">);</span><span class="w"></span>
<span class="w"> </span><span class="p">}</span><span class="w"></span>
</code></pre></div>

<p><strong>example.h</strong></p>
<div class="codehilite"><pre><span></span><code><span class="cm">/* File : example.h */</span><span class="w"></span>
<span class="err">#</span>include<span class="o">&lt;</span>stdio<span class="o">.</span>h<span class="o">&gt;</span><span class="w"></span>
<span class="w"> </span>extern<span class="w"> </span>unsigned<span class="w"> </span>int<span class="w"> </span>add<span class="o">(</span>unsigned<span class="w"> </span>int<span class="w"> </span>a<span class="o">,</span><span class="w"> </span>unsigned<span class="w"> </span>int<span class="w"> </span>b<span class="o">)</span><span class="err">;</span><span class="w"></span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>gcc  -fPIC -Wall -g -c example.c
gcc -shared -fPIC -o libexample.so example.o
</code></pre></div>

<p>Upload <code>libexample.so</code> to a S3 location.</p>
<p>pyspark code to be executed - <strong>py_c_call.py</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">ctypes</span> <span class="kn">import</span> <span class="n">CDLL</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>

    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>\
        <span class="o">.</span><span class="n">builder</span>\
        <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;py-c-so-example&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

    <span class="n">basedir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">))</span>
    <span class="n">libpath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">basedir</span><span class="p">,</span> <span class="s1">&#39;libexample.so&#39;</span><span class="p">)</span>
    <span class="n">sum_list</span> <span class="o">=</span> <span class="n">CDLL</span><span class="p">(</span><span class="n">libpath</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),(</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)]</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span><span class="s2">&quot;b&quot;</span><span class="p">]</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;total&#39;</span><span class="p">,</span> <span class="n">sum_list</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</code></pre></div>

<p><strong>Request:</strong>  </p>
<div class="codehilite"><pre><span></span><code>cat &gt; spark-python-in-s3-Clib.json &lt;&lt;EOF
{
  &quot;name&quot;: &quot;spark-python-in-s3-Clib&quot;, 
  &quot;virtualClusterId&quot;: &quot;&lt;virtual-cluster-id&gt;&quot;, 
  &quot;executionRoleArn&quot;: &quot;&lt;execution-role-arn&gt;&quot;, 
  &quot;releaseLabel&quot;: &quot;emr-6.2.0-latest&quot;, 
  &quot;jobDriver&quot;: {
    &quot;sparkSubmitJobDriver&quot;: {
      &quot;entryPoint&quot;: &quot;s3://&lt;s3 prefix&gt;/py_c_call.py&quot;, 
       &quot;sparkSubmitParameters&quot;: &quot;--files s3://&lt;s3 prefix&gt;/libexample.so --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2&quot;
    }
  }, 
  &quot;configurationOverrides&quot;: {
    &quot;applicationConfiguration&quot;: [
      {
        &quot;classification&quot;: &quot;spark-defaults&quot;, 
        &quot;properties&quot;: {
          &quot;spark.dynamicAllocation.enabled&quot;:&quot;false&quot;
         }
      }
    ], 
    &quot;monitoringConfiguration&quot;: {
      &quot;cloudWatchMonitoringConfiguration&quot;: {
        &quot;logGroupName&quot;: &quot;/emr-containers/jobs&quot;, 
        &quot;logStreamNamePrefix&quot;: &quot;demo&quot;
      }, 
      &quot;s3MonitoringConfiguration&quot;: {
        &quot;logUri&quot;: &quot;s3://joblogs&quot;
      }
    }
  }
}
EOF

aws emr-containers start-job-run --cli-input-json file:///spark-python-in-s3-Clib.json
</code></pre></div>

<p><strong>Configuration of interest:</strong><br />
<code>--files s3://&lt;s3 prefix&gt;/libexample.so</code> distributes the <code>libexample.so</code> to the working directory of all executors.<br />
Dynamic modules(.pyd, .so) can also be imported by bundling within  <a href="# Bundled as a .egg file">.egg</a> (<a href="https://issues.apache.org/jira/browse/SPARK-6764">SPARK-6764</a>), <a href="# Bundled as a .whl file">.whl</a> and <a href="# Bundled as a .pex file">.pex</a> files.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../../../../security/docs/spark/secrets/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Secrets" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Secrets
            </div>
          </div>
        </a>
      
      
        
        <a href="../../../../storage/docs/spark/ebs/" class="md-footer__link md-footer__link--next" aria-label="Next: EBS" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              EBS
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tabs"], "search": "../../../../assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.5a2dcb6a.min.js"></script>
      
    
    
  </body>
</html>